{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c715e",
   "metadata": {},
   "source": [
    "In this notebook, we will generate synthetic networks. The content is divided into two sections: the first covers \"canonical\" models from the network science literature while the second introduces an example of a contemporary model used in network neuroscience.\n",
    "\n",
    "For the canonical models, we use the networkx library -- these models (and others) already exist. The skilled programmer, looking for a challenge, might try implementing these models from scratch. None are particularly challenging, but require some forethought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de67e94",
   "metadata": {},
   "source": [
    "## Canonical models\n",
    "### Erdos-Renyi graphs\n",
    "The simplest model is the Erdos-Renyi (ER) random graph. This model generates networks with no structure but with derivable statistical properties. The edge between nodes $\\{i,j\\}$ forms independently and with probability $p$, a parameter that is held constant across all edges.\n",
    "\n",
    "Nodes' degrees in ER networks are binomially distributed. We show the degree distribution for the above network below. Any other structure in the network -- e.g. clustering, modularity, degree correlations -- reflect statistical fluctuations. The ER model is, as noted before, structureless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000   # number of nodes\n",
    "p = 0.01   # connection probability\n",
    "\n",
    "# generate graph object\n",
    "g_er = nx.erdos_renyi_graph(n,p)\n",
    "\n",
    "# calculate degree\n",
    "hist = nx.degree_histogram(g_er)\n",
    "bins = np.arange(0,len(hist),1)\n",
    "\n",
    "# plot degree distribution\n",
    "plt.bar(bins,hist/np.sum(hist))\n",
    "plt.xlabel('degree, $k$');\n",
    "plt.ylabel('$Pr(k)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9959675",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Coding challenge.** Oftentimes we want to compare networks of equal density. However, the ER graph is stochastic and, although its mean density is fixed, individual instances of the graph will have different numbers of edges. Suppose you wanted to generate an ER graph with a fixed density (number of edges/number of possible edges). How might you go about doing this? Note that the networkx package does not include this functionality.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5b7a0",
   "metadata": {},
   "source": [
    "### Watts-Strogatz model\n",
    "At the end of the 20th century, advances in computation and the ability to share data through the World Wide Web made it possible for scientists to study the structure of real-world networks. It was clear that real-world networks look very little like the classical network models, e.g. ER networks or regular, lattice graphs. Duncan Watts and Steven Strogatz noted that, specifically, real-world networks seemed to balance properties of those two network models: their path length was comparable to that of the ER network while their clustering coefficient was comparable to that of a lattice graph. At the time, the field lacked a model to explain the co-appearance of these two features.\n",
    "\n",
    "Watts \\& Strogatz proposed, in a now canonical paper, a model that explains these features of real world networks. Specifically, they imagined a network that begins as a lattice but, through the addition of a small number of \"shortcuts\" (randomly deleting and adding edges), reduces its path length to levels comparable to ER networks while preserving its high levels of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993eb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "n = 1000       # number of nodes\n",
    "k = 10         # degree of each node\n",
    "nsamples = 51  # how finely to sample rewiring probabilities\n",
    "\n",
    "# rewiring probabilites\n",
    "plist = np.logspace(-3,0,nsamples)\n",
    "\n",
    "# loop over all probabilities and calculate\n",
    "# clustering and path length\n",
    "c = np.zeros(nsamples)\n",
    "l = np.zeros(nsamples)\n",
    "for i,p in enumerate(plist):\n",
    "    g_ws = nx.watts_strogatz_graph(n,k,p)\n",
    "    c[i] = nx.average_clustering(g_ws)\n",
    "    l[i] = nx.average_shortest_path_length(g_ws)\n",
    "\n",
    "# plot curves\n",
    "plt.plot(np.log10(plist),c/c[0],label='clustering')\n",
    "plt.plot(np.log10(plist),l/l[0],label='path length')\n",
    "plt.legend()\n",
    "plt.xlabel('rewiring probability $p$');\n",
    "plt.ylabel('normalized measures');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f39b14",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Coding challenge.** The WS model highlights a broad range of $p$ values as having ``small-worldness.'' An alternative strategy is to define a small-world index. The most popular version defines this measure as: $S = \\frac{C}{C_r}/\\frac{L}{L_r}$, where $C$ and $L$ are the global clustering and path length of a network and $C_r$ and $L_r$ are those measures for a density-matched random networks (all connections randomized). Small-world networks should have much greater clustering than a random network, so the ratio $\\frac{C}{C_r} > 1$ while their path length should be comparable to that of a random network, so the ration $\\frac{L}{L_r} \\approx 1$. Thus, $S > 1$ for networks that have small-world features. Write a function for this index and use it to calculate the small-world index of WS networks as a function of $p$ in the above loop. Then plot the results.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790422f",
   "metadata": {},
   "source": [
    "### Barabasi-Albert model\n",
    "The WS model demonstrated that simple mechanisms -- in this case the addition of a little randomness -- can yield synthetic networks that stylistically capture features of real-world networks. However, the WS model misses other features of real-world networks, including heterogenous degree distributions and the presence of highly-connected hubs.\n",
    "\n",
    "In 1999 Laszlo Barabasi and Reka Albert reported a model that could recapitulate these features. Theirs was a \"growth model\" in which nodes are added to a network iteratively. As each node is added, it connects to existing nodes, preferring those with higher degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "n = 10000       # number of nodes\n",
    "m = 10          # each node makes this many connections to existing nodes\n",
    "\n",
    "# generate the network\n",
    "g_ba = nx.barabasi_albert_graph(n,m)\n",
    "\n",
    "# calculate degree\n",
    "hist = nx.degree_histogram(g_ba)\n",
    "bins = np.arange(0,len(hist),1)\n",
    "\n",
    "# plot degree distribution\n",
    "plt.scatter(np.log10(bins),np.log10(hist/np.sum(hist)))\n",
    "plt.xlabel('log degree, $k$');\n",
    "plt.ylabel('log $Pr(k)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8215d",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Coding challenge.** Fit a linear model to the data in the above plot. What is the slope? How does the slope change as you vary $n$ and $m$?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05169b78",
   "metadata": {},
   "source": [
    "### Stochastic blockmodels\n",
    "Still, other features were left unaccounted for, including \"community structure\", or the propensity for real-world networks to be divisible into meaningful sub-networks. The stochastic blockmodel addresses this gap. It imagines that each node, $i$, is assigned a cluster label, $c \\in \\{1 , \\ldots, K \\}$. The element $\\omega_{rs}$ of the cluster interaction matrix, $\\Omega \\in \\mathbb{R}^{K \\times K}$, determines the probability that two nodes assigned to clusters $r$ and $s$ will be connected to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "sz = [25,25,10,30]                 # size of each community\n",
    "prob = [                        # community-to-community connection probability\n",
    "        [0.00,0.40,0.20,0.25],\n",
    "        [0.50,0.00,0.01,0.40],\n",
    "        [0.00,0.10,0.80,0.20],\n",
    "        [0.50,0.00,0.20,0.00]\n",
    "       ]\n",
    "\n",
    "# generate the network \n",
    "g_sbm = nx.stochastic_block_model(sz, prob, directed=True)\n",
    "\n",
    "# visualize connectivity matrix\n",
    "a = nx.adjacency_matrix(g_sbm)\n",
    "a = a.todense()\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30dd16",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Coding challenge.** Although most applications imagine communities as being internally-dense and externally sparse--i.e. assortative--communities can interact through other motifs, including core-periphery and disassortatively. For instance, communities 3 and 4 in the above graph correspond to a core-periphery motif--community 3 is internally dense but weakly connects to community 4, whereas community 4 is internally sparse. Communities 1 and 2 form a disassortative motif, such that the between-community connection density is greater than either within-community density. More formally, if we have two communities--$r$ and $s$--we can classify their interactions using the following set of rules: assortative if $min(\\omega_{rr},\\omega_{ss}) > \\omega_{rs}$; core-periphery if $\\omega_{rr} > \\omega_{rs} > \\omega_{ss}$ or $\\omega_{ss} > \\omega_{rs} > \\omega_{rr}$; disassortative if $\\omega_{rs} > max(\\omega_{rr},\\omega_{ss})$. Write a function that considers all community pairs and classifies their interaction. How many interactions in the above network are assortative? Disassortative? Core-periphery?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966024c7",
   "metadata": {},
   "source": [
    "### Geometric random graphs\n",
    "Other networks are spatially embedded so that link formation depends on the proximity of nodes to one another. Geometric random graphs model these dependencies. Here we illustrate this by positioning $n$ nodes within a unit circle and connect nodes separated by a radius of $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f78671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "n = 100                               # number of nodes\n",
    "coor = np.random.uniform(-1,1,[n,2])  # coordinates\n",
    "r = 0.5                               # radius for connecting nodes\n",
    "\n",
    "# create list of nodes and positions\n",
    "x = []\n",
    "for i in range(n):\n",
    "    y = [i,{\"pos\": (coor[i,0],coor[i,1])}]\n",
    "    x.append(y)\n",
    "    \n",
    "# create graph and add nodes\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(x)\n",
    "\n",
    "# create graph from nearest nodes\n",
    "H = nx.from_edgelist(nx.geometric_edges(G, radius=r))\n",
    "\n",
    "# plot network\n",
    "nx.draw_networkx(H,pos=coor,node_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30860d",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Coding challenge.** In the slides I note that there are alternative geometric models including one in which the probability of nodes $i$ and $j$ connecting to one another is proportional to $Pr(A_{ij} = 1) = \\alpha e^{-\\beta D_{ij}}$. Write code that implements this model. Perform a grid search over the parameters $\\alpha$ and $\\beta$ and measure some basic network properties, e.g. number of connections, modularity, clustering, etc. Plot their values as a heat map. How do these network measures depend on the parameter values?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a8ba4",
   "metadata": {},
   "source": [
    "## Brain network models\n",
    "### Quasi-dynamic model from Vertes et al 2012 and Betzel et al 2016\n",
    "These papers proposed a two-term model of brain networks. The model is quasi-dynamic in that edges are added sequentially (over \"time\") though the time units are meaningless and the process is not intended to be a model of brain network growth, development, and evolution.\n",
    "\n",
    "Briefly, the model takes the following form:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Pr(A_{ij} = 1) \\propto D_{ij}^{\\eta} \\times K_{ij}^{\\gamma}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In this expression, $i$ and $j$ represent two nodes; $D_{ij}$ is the distance between their centroids; $K_{ij}$ is the topological term; $\\eta$ and $\\gamma$ are parameters that need to be fit to data.\n",
    "\n",
    "The algorithm works as follows:\n",
    "1. Initialize a (near-)empty adjacency matrix.\n",
    "2. Calculate $Pr(A_{ij})$ for all pairs of nodes.\n",
    "3. Probabilistically select one of those pairs and add it to the network.\n",
    "4. Update the topological term.\n",
    "5. Update $Pr(A_{ij})$.\n",
    "6. Repeat steps 1-5 until $m$ edges have been added.\n",
    "\n",
    "This procedure yields a synthetic network. We calculate for both the observed and synthetic network the following measure:\n",
    "1. Node degree\n",
    "2. Node clustering\n",
    "3. Node betweenness centrality\n",
    "4. Edge length (distance)\n",
    "\n",
    "For each measure, we compare the observed and synthetic distribution using the Kolmogorov-Smirnov statistic. We define the energy of the synthetic network as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E = max(KS_k,KS_b,KS_c,KS_e)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Low values of $E$ correspond to better fits to the observed network.\n",
    "\n",
    "#### Pure spatial model\n",
    "The simplest version of this generative model is the purely spatial model, wherein $\\gamma = 0$ and $K_{ij}^\\gamma = 1$. The probability therefore resolves to $Pr(A_{ij} = 1) \\propto D_{ij}^{\\eta} \\times 1$ and depends only on the separate between $i$ and $j$.\n",
    "\n",
    "Here we illustrate how to implement this network and compare it against an empirical connectome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4393fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load connectome\n",
    "edgelist = np.loadtxt('data/edge_list.txt',delimiter=',')\n",
    "nr,nc = np.shape(edgelist)\n",
    "\n",
    "# load coordinates\n",
    "coor = np.loadtxt('data/coor.txt',delimiter=',')\n",
    "n,dummy = np.shape(coor)\n",
    "\n",
    "# define matrix\n",
    "a = np.zeros([n,n])\n",
    "for e in edgelist:\n",
    "    a[int(e[0] - 1),int(e[1] - 1)] = 1\n",
    "    \n",
    "# binary connectivity marix\n",
    "a = np.triu(a,1) + np.transpose(np.triu(a,1))\n",
    "\n",
    "# pairwise distance matrix\n",
    "d = np.zeros([n,n])\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        d[i,j] = sum((coor[i] - coor[j])**2)**(0.5)\n",
    "        \n",
    "        \n",
    "# calculate example probability matrix\n",
    "eta = -2\n",
    "e = np.power(d,eta)\n",
    "\n",
    "# plot connectivity matrix\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(a)\n",
    "plt.xlabel('nodes')\n",
    "plt.ylabel('nodes')\n",
    "\n",
    "# plot distance matrix\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(d)\n",
    "plt.xlabel('nodes')\n",
    "plt.ylabel('nodes')\n",
    "\n",
    "# plot probability matrix\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(e)\n",
    "plt.xlabel('nodes')\n",
    "plt.ylabel('nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "eta = -5\n",
    "mtgt = int(np.sum(a>0)/2)\n",
    "amdl = np.zeros([n,n])\n",
    "m = 0\n",
    "\n",
    "# upper triangle indices\n",
    "uv = np.triu_indices(n,k=1)\n",
    "\n",
    "# get relative probability\n",
    "p = np.power(d[uv],eta)\n",
    "\n",
    "# normalize\n",
    "p = p/sum(p)\n",
    "\n",
    "# number of total possible edges\n",
    "nedges = len(p)\n",
    "\n",
    "# add edges\n",
    "for m in range(mtgt):\n",
    "    \n",
    "    # choose an edge at random proportional to values in p;\n",
    "    # then set that value to 0 so it doesn't get selected\n",
    "    # again and then renormalize\n",
    "    r = np.random.choice(nedges,1,p=p)\n",
    "    p[r] = 0\n",
    "    p = p/sum(p)\n",
    "    \n",
    "    # add the edge\n",
    "    amdl[uv[0][r],uv[1][r]] = 1\n",
    "    amdl[uv[1][r],uv[0][r]] = 1\n",
    "    \n",
    "plt.imshow(amdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38981ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make graph objects for observed and model\n",
    "g_observed = nx.Graph(a)\n",
    "g_mdl = nx.Graph(amdl)\n",
    "\n",
    "# calculate graph statistics\n",
    "# degree\n",
    "d_observed = nx.degree(g_observed)\n",
    "d_mdl = nx.degree(g_mdl)\n",
    "\n",
    "# clustering\n",
    "c_observed = nx.clustering(g_observed)\n",
    "c_mdl = nx.clustering(g_mdl)\n",
    "\n",
    "# betweenness\n",
    "b_observed = nx.betweenness_centrality(g_observed)\n",
    "b_mdl = nx.betweenness_centrality(g_mdl)\n",
    "\n",
    "# transform into lists from graph annotation objects\n",
    "deg_observed = []\n",
    "deg_mdl = []\n",
    "\n",
    "clu_observed = []\n",
    "clu_mdl = []\n",
    "\n",
    "btw_observed = []\n",
    "btw_mdl = []\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    deg_observed.append(d_observed[i])\n",
    "    deg_mdl.append(d_mdl[i])\n",
    "    \n",
    "    clu_observed.append(c_observed[i])\n",
    "    clu_mdl.append(c_mdl[i])\n",
    "    \n",
    "    btw_observed.append(b_observed[i])\n",
    "    btw_mdl.append(b_mdl[i])\n",
    "    \n",
    "# calculate ks statistics\n",
    "ksk,dummy = stats.ks_2samp(deg_observed,deg_mdl)\n",
    "ksc,dummy = stats.ks_2samp(clu_observed,clu_mdl)\n",
    "ksb,dummy = stats.ks_2samp(btw_observed,btw_mdl)\n",
    "\n",
    "# calcualte edge length distributions\n",
    "e_observed = d[a > 0]\n",
    "e_mdl = d[amdl > 0]\n",
    "kse,dummy = stats.ks_2samp(e_observed,e_mdl)\n",
    "\n",
    "# make a list of all ks stats\n",
    "ks_all = [ksk,ksc,ksb,kse]\n",
    "\n",
    "# combine into an energy\n",
    "energy = max(ks_all)\n",
    "print(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af867c4b",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Coding challenge.** Keep $\\eta$ fixed at one value, but write a loop that generates many synthetic networks and evaluates each of their energies. Plot the mean and standard deviation of energy as a function of the number of samples. How many samples do you need before the variance becomes small (you get to decide what it means to be small).</font>\n",
    "\n",
    "<font color=\"red\">Write two functions from the above code. The first should be called <code>genmod_sptl</code> and takes as input the parameters <code>eta</code>, <code>d</code>, and <code>mtgt</code>. It should return the adjacency matrix for the synthetic network. The second function should be called <code>calculate_energy</code> and should take as input the adjacency matrices for the empirical and synthetic networks, returning the four KS statistics and the energy of the synthetic network.</font>\n",
    "\n",
    "<font color=\"red\">Now, systematically vary the value of $\\eta$ from -4 to 0. How does energy change as you vary $\\eta$? Can you identify the ``sweet spot'' where $E$ appears close to a local minimum?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f988381",
   "metadata": {},
   "source": [
    "#### Nearest-neighbor + spatial model\n",
    "We can make this generative model more complicated by setting $\\gamma \\ne 0$. Here, we will set $K_{ij} = \\sum_l A_{il} A_{lj}$, which is equal to the number of neighbors shared by nodes $i$ and $j$. The probability therefore resolves to $Pr(A_{ij} = 1) \\propto D_{ij}^{\\eta} \\times K_{ij}^\\gamma$ and depends not nly on the separate between $i$ and $j$, but the extent to which their neighborhoods overlap.\n",
    "\n",
    "Here we illustrate how to implement this network and compare it against an empirical connectome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "eta = -2.5\n",
    "gam = 2\n",
    "mtgt = int(np.sum(a>0)/2)\n",
    "amdl = np.zeros([n,n])\n",
    "m = 0\n",
    "\n",
    "# upper triangle indices\n",
    "uv = np.triu_indices(n,k=1)\n",
    "\n",
    "# common neighbors\n",
    "b = np.ones([n,n])\n",
    "\n",
    "# get relative probability\n",
    "p = np.power(d[uv],eta)*np.power(b[uv],gam)\n",
    "\n",
    "# normalize\n",
    "p = p/sum(p)\n",
    "\n",
    "# number of total possible edges\n",
    "nedges = len(p)\n",
    "\n",
    "# add edges\n",
    "indx = []\n",
    "for m in range(mtgt):\n",
    "    \n",
    "    # choose an edge at random proportional to values in p;\n",
    "    # then set that value to 0 so it doesn't get selected\n",
    "    # again and then renormalize\n",
    "    r = np.random.choice(nedges,1,p=p)\n",
    "    \n",
    "    # add the edge\n",
    "    amdl[uv[0][r],uv[1][r]] = 1\n",
    "    amdl[uv[1][r],uv[0][r]] = 1\n",
    "    \n",
    "    # update the number of shared neighbors (and add 1 to avoid 0's)\n",
    "    b = np.matmul(amdl,amdl) + 1\n",
    "    \n",
    "    # update list of existing edges by index\n",
    "    indx.append(r)\n",
    "    \n",
    "    # calculate relative probability\n",
    "    p = np.power(d[uv],eta)*np.power(b[uv],gam)\n",
    "    \n",
    "    # set equal to zero existing edges and normalize\n",
    "    p[indx] = 0\n",
    "    p = p/sum(p)\n",
    "    \n",
    "    \n",
    "plt.imshow(amdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make graph objects for observed and model\n",
    "g_observed = nx.Graph(a)\n",
    "g_mdl = nx.Graph(amdl)\n",
    "\n",
    "# calculate graph statistics\n",
    "# degree\n",
    "d_observed = nx.degree(g_observed)\n",
    "d_mdl = nx.degree(g_mdl)\n",
    "\n",
    "# clustering\n",
    "c_observed = nx.clustering(g_observed)\n",
    "c_mdl = nx.clustering(g_mdl)\n",
    "\n",
    "# betweenness\n",
    "b_observed = nx.betweenness_centrality(g_observed)\n",
    "b_mdl = nx.betweenness_centrality(g_mdl)\n",
    "\n",
    "# transform into lists from graph annotation objects\n",
    "deg_observed = []\n",
    "deg_mdl = []\n",
    "\n",
    "clu_observed = []\n",
    "clu_mdl = []\n",
    "\n",
    "btw_observed = []\n",
    "btw_mdl = []\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    deg_observed.append(d_observed[i])\n",
    "    deg_mdl.append(d_mdl[i])\n",
    "    \n",
    "    clu_observed.append(c_observed[i])\n",
    "    clu_mdl.append(c_mdl[i])\n",
    "    \n",
    "    btw_observed.append(b_observed[i])\n",
    "    btw_mdl.append(b_mdl[i])\n",
    "    \n",
    "# calculate ks statistics\n",
    "ksk,dummy = stats.ks_2samp(deg_observed,deg_mdl)\n",
    "ksc,dummy = stats.ks_2samp(clu_observed,clu_mdl)\n",
    "ksb,dummy = stats.ks_2samp(btw_observed,btw_mdl)\n",
    "\n",
    "# calcualte edge length distributions\n",
    "e_observed = d[a > 0]\n",
    "e_mdl = d[amdl > 0]\n",
    "kse,dummy = stats.ks_2samp(e_observed,e_mdl)\n",
    "\n",
    "# make a list of all ks stats\n",
    "ks_all = [ksk,ksc,ksb,kse]\n",
    "\n",
    "# combine into an energy\n",
    "energy = max(ks_all)\n",
    "print(energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c3953",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Coding challenge.** Systematically vary both $\\eta$ and $\\gamma$; this exploration will trace out a 2D parameter space with energy defined at each point. Can you identify the ``sweet spot'' where $E$ appears close to a local minimum? Is the best 2-parameter model better than the 1-parameter spatial model? By how much?</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
